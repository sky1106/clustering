{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import color, transform   # lib: scikit-image\n",
    "import keras\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, cifar_noniid, fashion_mnist_iid, fashion_mnist_noniid\n",
    "from utils.lsh import LSHAlgo\n",
    "from models.Fed import FedAvg\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "user_feats = []\n",
    "print('Loading dataset...')\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "dict_users = mnist_noniid(dataset_train, 100, case=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(800, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_fed(dataset_train, dataset_test, dict_users, type_exp = 'base'):\n",
    "    img_size = dataset_train[0][0].shape\n",
    "\n",
    "    dict_clusters = {}\n",
    "    if type_exp == 'cluster' or type_exp == 'lsh-cluster':\n",
    "        # feature map\n",
    "        print('Featuring...')\n",
    "        input_shape = (max(img_size[1], 32), max(img_size[2], 32), max(img_size[0], 3))\n",
    "        model1 = keras.applications.resnet.ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "\n",
    "        if len(user_feats):\n",
    "            pass\n",
    "        else:\n",
    "            for idx_user in dict_users:\n",
    "                print('User', idx_user, 'featuring...')\n",
    "                user_images = []\n",
    "                for idx in dict_users[idx_user]:\n",
    "                    image = dataset_train[idx][0].numpy()\n",
    "                    image = color.gray2rgb(image)[0]\n",
    "                    image = transform.resize(image, (32, 32))\n",
    "                    user_images.append(image)\n",
    "\n",
    "                pred = model1.predict([user_images])\n",
    "                feats = np.mean([data[0][0] for data in pred], axis=0)\n",
    "                user_feats.append(feats)\n",
    "\n",
    "        if type_exp == 'lsh-cluster':\n",
    "            # 局部敏感哈希\n",
    "            print('LSH...')\n",
    "            lsh = LSHAlgo(feat_dim=len(user_feats[0]), code_dim=512) # code_dim: 输出维度\n",
    "            user_feats1 = lsh.run(user_feats)\n",
    "        else:\n",
    "            # 普通降维\n",
    "            print('PCA...')\n",
    "            pca = PCA(n_components=50, random_state=728)\n",
    "            user_feats1 = pca.fit_transform(user_feats)\n",
    "\n",
    "        # 聚类 users\n",
    "        print('Clustering...')\n",
    "        kmeans = KMeans(n_clusters=10, random_state=728)\n",
    "        kmeans.fit(user_feats1)\n",
    "\n",
    "        for idx_user, label in enumerate(kmeans.labels_):\n",
    "            if label in dict_clusters:\n",
    "                dict_clusters[label].append(idx_user)\n",
    "            else:\n",
    "                dict_clusters[label] = [idx_user]\n",
    "        print('Clustering finished.')\n",
    "        print('Dict of cluster - users: ', dict_clusters)\n",
    "\n",
    "\n",
    "    # build model\n",
    "    net_glob = CNNMnist().to('cuda:0')\n",
    "    print(net_glob)\n",
    "    net_glob.train()\n",
    "\n",
    "\n",
    "    # batch training\n",
    "\n",
    "    return batch_train(type_exp, net_glob, dataset_train, dataset_test, dict_users, dict_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(type_exp, net_glob, dataset_train, dataset_test, dict_users, dict_clusters):\n",
    "    loss_train_batch = []\n",
    "    acc_test_batch = []\n",
    "\n",
    "    for big_iter in range(5):\n",
    "        print('Iteration ', big_iter)\n",
    "\n",
    "        # copy weights\n",
    "        net_glob_copy = copy.deepcopy(net_glob)\n",
    "\n",
    "        # training\n",
    "        loss_train = []\n",
    "        acc_test = []\n",
    "\n",
    "        for iter in range(5):\n",
    "            one_loss_train, one_acc_test = train_one_round(iter, type_exp, net_glob_copy, dataset_train, dataset_test, dict_users, dict_clusters)\n",
    "            loss_train.append(one_loss_train)\n",
    "            acc_test.append(one_acc_test)\n",
    "\n",
    "        loss_train_batch.append(loss_train)\n",
    "        acc_test_batch.append(acc_test)\n",
    "\n",
    "    loss_train_avg = np.mean(loss_train_batch, axis=0)\n",
    "    acc_test_avg = np.mean(acc_test_batch, axis=0)\n",
    "\n",
    "    loss_train_std = np.std(loss_train_batch, axis=0)\n",
    "    acc_test_std = np.std(acc_test_batch, axis=0)\n",
    "\n",
    "    return loss_train_avg, acc_test_avg, loss_train_std, acc_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=128)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if False:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_round(iter, type_exp, net_glob, dataset_train, dataset_test, dict_users, dict_clusters):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    if type_exp == 'cluster' or type_exp == 'lsh-cluster':\n",
    "        # 预先聚类的情况\n",
    "        idxs_users = []\n",
    "        for idx_cluster in dict_clusters:\n",
    "            idxs_users += list(np.random.choice(list(dict_clusters[idx_cluster]), 1, replace=False))\n",
    "    else:\n",
    "        m = max(int(0.1 * 100), 1)\n",
    "        idxs_users = np.random.choice(range(100), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob_copy\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss & acc\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    one_acc_test, one_loss_test = test_img(net_glob, dataset_test)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Test accuracy {:.3f}'.format(iter, loss_avg, one_acc_test))\n",
    "\n",
    "    return loss_avg, one_acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalUpdate(object):\n",
    "    def __init__(self, dataset=None, idxs=None):\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=50, shuffle=True)\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        # train and update\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "        epoch_loss = []\n",
    "        for iter in range(5):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to('cuda:0'), labels.to('cuda:0')\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images)\n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if False and batch_idx % 10 == 0:\n",
    "                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n",
    "                               100. * batch_idx / len(self.ldr_train), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, data_std, ylabel):\n",
    "    plt.figure()    \n",
    "    # colour = ['darkblue','darkred','darkgreen','black','darkmagenta','darkorange','darkcyan']\n",
    "    # ecolour = ['cornflowerblue','lightcoral','lightgreen','gray','magenta','bisque','cyan']\n",
    "    # i = 0\n",
    "    for label in data:\n",
    "        arr1 = data[label][0:len(data[label]):10]\n",
    "        print(len(data[label]), len(arr1), arr1)\n",
    "        plt.plot(range(0, 3000, 10), arr1, label=label, linestyle=':')\n",
    "        # i = i + 1\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig('./test/x.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin time:  2020-06-28-14-51-29\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Iteration  0\n",
      "Round   0, Average loss 0.210, Test accuracy 10.320\n",
      "Round   1, Average loss 0.095, Test accuracy 16.230\n",
      "Round   2, Average loss 0.101, Test accuracy 9.800\n",
      "Round   3, Average loss 0.170, Test accuracy 16.470\n",
      "Round   4, Average loss 0.147, Test accuracy 17.200\n",
      "Iteration  1\n",
      "Round   0, Average loss 0.211, Test accuracy 12.040\n",
      "Round   1, Average loss 0.096, Test accuracy 10.100\n",
      "Round   2, Average loss 0.084, Test accuracy 10.910\n",
      "Round   3, Average loss 0.084, Test accuracy 17.920\n",
      "Round   4, Average loss 0.090, Test accuracy 10.320\n",
      "Iteration  2\n",
      "Round   0, Average loss 0.222, Test accuracy 17.230\n",
      "Round   1, Average loss 0.115, Test accuracy 9.580\n",
      "Round   2, Average loss 0.125, Test accuracy 10.320\n",
      "Round   3, Average loss 0.090, Test accuracy 12.160\n",
      "Round   4, Average loss 0.120, Test accuracy 13.110\n",
      "Iteration  3\n",
      "Round   0, Average loss 0.281, Test accuracy 10.320\n",
      "Round   1, Average loss 0.120, Test accuracy 17.010\n",
      "Round   2, Average loss 0.085, Test accuracy 9.800\n",
      "Round   3, Average loss 0.163, Test accuracy 18.860\n",
      "Round   4, Average loss 0.119, Test accuracy 11.350\n",
      "Iteration  4\n",
      "Round   0, Average loss 0.278, Test accuracy 13.930\n",
      "Round   1, Average loss 0.100, Test accuracy 20.670\n",
      "Round   2, Average loss 0.152, Test accuracy 17.510\n",
      "Round   3, Average loss 0.084, Test accuracy 18.850\n",
      "Round   4, Average loss 0.050, Test accuracy 16.360\n",
      "Featuring...\n",
      "User 0 featuring...\n",
      "User 1 featuring...\n",
      "User 2 featuring...\n",
      "User 3 featuring...\n",
      "User 4 featuring...\n",
      "User 5 featuring...\n",
      "User 6 featuring...\n",
      "User 7 featuring...\n",
      "User 8 featuring...\n",
      "User 9 featuring...\n",
      "User 10 featuring...\n",
      "User 11 featuring...\n",
      "User 12 featuring...\n",
      "User 13 featuring...\n",
      "User 14 featuring...\n",
      "User 15 featuring...\n",
      "User 16 featuring...\n",
      "User 17 featuring...\n",
      "User 18 featuring...\n",
      "User 19 featuring...\n",
      "User 20 featuring...\n",
      "User 21 featuring...\n",
      "User 22 featuring...\n",
      "User 23 featuring...\n",
      "User 24 featuring...\n",
      "User 25 featuring...\n",
      "User 26 featuring...\n",
      "User 27 featuring...\n",
      "User 28 featuring...\n",
      "User 29 featuring...\n",
      "User 30 featuring...\n",
      "User 31 featuring...\n",
      "User 32 featuring...\n",
      "User 33 featuring...\n",
      "User 34 featuring...\n",
      "User 35 featuring...\n",
      "User 36 featuring...\n",
      "User 37 featuring...\n",
      "User 38 featuring...\n",
      "User 39 featuring...\n",
      "User 40 featuring...\n",
      "User 41 featuring...\n",
      "User 42 featuring...\n",
      "User 43 featuring...\n",
      "User 44 featuring...\n",
      "User 45 featuring...\n",
      "User 46 featuring...\n",
      "User 47 featuring...\n",
      "User 48 featuring...\n",
      "User 49 featuring...\n",
      "User 50 featuring...\n",
      "User 51 featuring...\n",
      "User 52 featuring...\n",
      "User 53 featuring...\n",
      "User 54 featuring...\n",
      "User 55 featuring...\n",
      "User 56 featuring...\n",
      "User 57 featuring...\n",
      "User 58 featuring...\n",
      "User 59 featuring...\n",
      "User 60 featuring...\n",
      "User 61 featuring...\n",
      "User 62 featuring...\n",
      "User 63 featuring...\n",
      "User 64 featuring...\n",
      "User 65 featuring...\n",
      "User 66 featuring...\n",
      "User 67 featuring...\n",
      "User 68 featuring...\n",
      "User 69 featuring...\n",
      "User 70 featuring...\n",
      "User 71 featuring...\n",
      "User 72 featuring...\n",
      "User 73 featuring...\n",
      "User 74 featuring...\n",
      "User 75 featuring...\n",
      "User 76 featuring...\n",
      "User 77 featuring...\n",
      "User 78 featuring...\n",
      "User 79 featuring...\n",
      "User 80 featuring...\n",
      "User 81 featuring...\n",
      "User 82 featuring...\n",
      "User 83 featuring...\n",
      "User 84 featuring...\n",
      "User 85 featuring...\n",
      "User 86 featuring...\n",
      "User 87 featuring...\n",
      "User 88 featuring...\n",
      "User 89 featuring...\n",
      "User 90 featuring...\n",
      "User 91 featuring...\n",
      "User 92 featuring...\n",
      "User 93 featuring...\n",
      "User 94 featuring...\n",
      "User 95 featuring...\n",
      "User 96 featuring...\n",
      "User 97 featuring...\n",
      "User 98 featuring...\n",
      "User 99 featuring...\n",
      "PCA...\n",
      "Clustering...\n",
      "Clustering finished.\n",
      "Dict of cluster - users:  {9: [0, 6, 13, 48, 50, 65, 66, 73, 79, 97], 1: [1, 12, 18, 25, 30, 31, 47, 53, 81, 87, 88], 8: [2, 16, 37, 45, 58, 68, 69, 72, 78, 96], 5: [3, 11, 22, 23, 39, 40, 61, 70, 82, 95], 0: [4, 9, 21, 32, 44, 63, 71, 80, 93, 94], 2: [5, 15, 19, 20, 27, 41, 57, 83, 86, 89], 7: [7, 8, 10, 24, 28, 52, 64, 67, 98, 99], 3: [14, 29, 33, 34, 46, 54, 56, 60, 84, 92], 6: [17, 38, 42, 49, 51, 55, 75, 76, 85, 90], 4: [26, 35, 36, 43, 59, 62, 74, 77, 91]}\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Iteration  0\n",
      "Round   0, Average loss 0.302, Test accuracy 10.320\n",
      "Round   1, Average loss 0.123, Test accuracy 10.390\n",
      "Round   2, Average loss 0.085, Test accuracy 16.160\n",
      "Round   3, Average loss 0.112, Test accuracy 23.940\n",
      "Round   4, Average loss 0.103, Test accuracy 28.810\n",
      "Iteration  1\n",
      "Round   0, Average loss 0.282, Test accuracy 10.160\n",
      "Round   1, Average loss 0.122, Test accuracy 14.700\n",
      "Round   2, Average loss 0.090, Test accuracy 14.050\n",
      "Round   3, Average loss 0.090, Test accuracy 27.290\n",
      "Round   4, Average loss 0.059, Test accuracy 30.590\n",
      "Iteration  2\n",
      "Round   0, Average loss 0.258, Test accuracy 10.100\n",
      "Round   1, Average loss 0.111, Test accuracy 10.910\n",
      "Round   2, Average loss 0.067, Test accuracy 23.870\n",
      "Round   3, Average loss 0.062, Test accuracy 16.290\n",
      "Round   4, Average loss 0.058, Test accuracy 42.330\n",
      "Iteration  3\n",
      "Round   0, Average loss 0.278, Test accuracy 10.130\n",
      "Round   1, Average loss 0.104, Test accuracy 9.750\n",
      "Round   2, Average loss 0.068, Test accuracy 17.750\n",
      "Round   3, Average loss 0.113, Test accuracy 17.370\n",
      "Round   4, Average loss 0.058, Test accuracy 22.680\n",
      "Iteration  4\n",
      "Round   0, Average loss 0.255, Test accuracy 10.100\n",
      "Round   1, Average loss 0.123, Test accuracy 10.160\n",
      "Round   2, Average loss 0.087, Test accuracy 23.880\n",
      "Round   3, Average loss 0.078, Test accuracy 18.630\n",
      "Round   4, Average loss 0.058, Test accuracy 37.870\n",
      "Featuring...\n",
      "LSH...\n",
      "Clustering...\n",
      "Clustering finished.\n",
      "Dict of cluster - users:  {8: [0, 6, 13, 48, 50, 65, 66, 73, 79, 97], 1: [1, 12, 18, 25, 30, 31, 47, 53, 81, 87, 88], 2: [2, 16, 37, 45, 58, 68, 69, 72, 78, 96], 5: [3, 11, 22, 23, 39, 40, 61, 70, 82, 95], 0: [4, 9, 21, 32, 44, 63, 71, 80, 93, 94], 3: [5, 15, 19, 20, 27, 41, 57, 83, 86, 89], 4: [7, 8, 10, 24, 28, 52, 64, 67, 98, 99], 6: [14, 29, 33, 34, 46, 54, 56, 60, 84, 92], 9: [17, 38, 42, 49, 51, 55, 75, 76, 85, 90], 7: [26, 35, 36, 43, 59, 62, 74, 77, 91]}\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Iteration  0\n",
      "Round   0, Average loss 0.208, Test accuracy 21.210\n",
      "Round   1, Average loss 0.097, Test accuracy 11.620\n",
      "Round   2, Average loss 0.067, Test accuracy 30.920\n",
      "Round   3, Average loss 0.085, Test accuracy 29.790\n",
      "Round   4, Average loss 0.078, Test accuracy 16.330\n",
      "Iteration  1\n",
      "Round   0, Average loss 0.231, Test accuracy 11.400\n",
      "Round   1, Average loss 0.069, Test accuracy 24.890\n",
      "Round   2, Average loss 0.060, Test accuracy 28.670\n",
      "Round   3, Average loss 0.088, Test accuracy 26.710\n",
      "Round   4, Average loss 0.084, Test accuracy 40.510\n",
      "Iteration  2\n",
      "Round   0, Average loss 0.204, Test accuracy 16.040\n",
      "Round   1, Average loss 0.068, Test accuracy 20.740\n",
      "Round   2, Average loss 0.060, Test accuracy 38.390\n",
      "Round   3, Average loss 0.059, Test accuracy 20.480\n",
      "Round   4, Average loss 0.092, Test accuracy 10.010\n",
      "Iteration  3\n",
      "Round   0, Average loss 0.202, Test accuracy 17.110\n",
      "Round   1, Average loss 0.069, Test accuracy 20.720\n",
      "Round   2, Average loss 0.090, Test accuracy 16.450\n",
      "Round   3, Average loss 0.086, Test accuracy 23.230\n",
      "Round   4, Average loss 0.080, Test accuracy 25.940\n",
      "Iteration  4\n",
      "Round   0, Average loss 0.202, Test accuracy 16.910\n",
      "Round   1, Average loss 0.121, Test accuracy 12.250\n",
      "Round   2, Average loss 0.082, Test accuracy 17.860\n",
      "Round   3, Average loss 0.086, Test accuracy 28.730\n",
      "Round   4, Average loss 0.087, Test accuracy 24.400\n",
      "{'base': array([0.24010662, 0.10528759, 0.10958549, 0.11822743, 0.10517779]), 'cluster': array([0.27510488, 0.11653744, 0.07944274, 0.09097647, 0.06733475]), 'lsh-cluster': array([0.20930415, 0.08474278, 0.07181644, 0.08091459, 0.08417415])} {'base': array([12.768   , 14.717999, 11.667999, 16.852001, 13.668001],\n",
      "      dtype=float32), 'cluster': array([10.162, 11.182, 19.142, 20.704, 32.456], dtype=float32), 'lsh-cluster': array([16.534   , 18.044   , 26.457998, 25.787998, 23.438   ],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print('begin time: ', datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "labels = ['base', 'cluster', 'lsh-cluster']\n",
    "dict_train_loss = {}\n",
    "dict_acc_test = {}\n",
    "dict_std_train_loss = {}\n",
    "dict_std_acc_test = {}\n",
    "for label in labels:\n",
    "    dict_train_loss[label], dict_acc_test[label], dict_std_train_loss[label], dict_std_acc_test[label] = run_fed(dataset_train, dataset_test, dict_users, type_exp = label)\n",
    "print(dict_train_loss, dict_acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(dict_train_loss, dict_std_train_loss, 'train_loss')\n",
    "plot(dict_acc_test, dict_std_acc_test, 'test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save finished\n"
     ]
    }
   ],
   "source": [
    "with open(r'./test/test.txt', 'a') as f:\n",
    "    for label in dict_acc_test:\n",
    "        f.write(label)\n",
    "        f.write(' ')\n",
    "        for item in dict_acc_test[label]:\n",
    "            item1 = str(item)\n",
    "            f.write(item1)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "print('save finished')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# for label in dict_acc_test:\n",
    "#     print(label)\n",
    "#     for item in dict_acc_test[label]:\n",
    "#         print(item)\n",
    "\n",
    "\n",
    "# for item in dict_acc_test.items():\n",
    "#     for i in range(len(item)):\n",
    "#         str1 = item[i]\n",
    "#         print(str1,end=' ')\n",
    "#         with open(r'./test/test.txt', 'a') as f:\n",
    "#             f.write(str1)\n",
    "#             f.write('\\r\\t')\n",
    "#         print('finish saving')    \n",
    "#         f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': array([12.768   , 14.717999, 11.667999, 16.852001, 13.668001],\n",
       "       dtype=float32),\n",
       " 'cluster': array([10.162, 11.182, 19.142, 20.704, 32.456], dtype=float32),\n",
       " 'lsh-cluster': array([16.534   , 18.044   , 26.457998, 25.787998, 23.438   ],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_acc_test1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4691c1e2c20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'./test/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_acc_test1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_acc_test1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_acc_test1' is not defined"
     ]
    }
   ],
   "source": [
    "with open(r'./test/test.txt', 'a') as f:\n",
    "    for label in dict_acc_test1:\n",
    "        f.write(label)\n",
    "        f.write(' ')\n",
    "        for item in dict_acc_test1[label]:\n",
    "            item1 = str(item)\n",
    "            f.write(item1)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "print('save finished')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = []\n",
    "items2 = []\n",
    "with open(r'./test/all_test_acc_cifar_cnn_1_3_3000_iidFalse_2020-06-27-09-40-48.txt', 'r') as f:\n",
    "    for item in f.readlines():\n",
    "        item1 = np.array(item.split())\n",
    "        label1 = item1[0]\n",
    "        labels1.append(label1)\n",
    "        item2 = np.delete(item1, 0).astype('float32')\n",
    "        items2.append(item2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': array([ 9.776667, 10.66    , 10.      , ..., 44.98    , 47.52    ,\n",
      "       41.88667 ], dtype=float32), 'cluster': array([10.      , 10.      , 11.276668, ..., 50.556667, 51.15333 ,\n",
      "       51.55    ], dtype=float32), 'lsh-cluster': array([ 9.996667, 10.166667, 10.496667, ..., 51.186665, 50.87    ,\n",
      "       52.18333 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "dict_acc_test1 = {}\n",
    "i=0\n",
    "for label in labels1:\n",
    "#     print(label)\n",
    "#     print(i)\n",
    "#     print(items2[i])\n",
    "    dict_acc_test1[label] = items2[i]\n",
    "    i = i+1\n",
    "print(dict_acc_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base', 'cluster', 'lsh-cluster']\n"
     ]
    }
   ],
   "source": [
    "print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 300 [ 9.776667  10.        10.163333  10.94      10.        11.183333\n",
      " 13.170001  11.863334  13.143333  13.296666  14.91      14.1466675\n",
      " 13.273334  15.953334  13.44      17.74      15.81      17.933334\n",
      " 17.58      16.143333  19.336666  17.363333  19.199999  15.686667\n",
      " 17.39      14.996667  20.846666  17.28      13.91      20.433332\n",
      " 15.2       16.699999  18.99      20.366667  22.063334  17.546667\n",
      " 21.873335  20.426668  18.956667  22.486666  23.14      22.566666\n",
      " 24.766668  23.836664  23.076666  21.783333  20.33      25.306665\n",
      " 21.590002  22.383333  20.073334  22.476667  23.39      23.710001\n",
      " 24.51333   24.466667  27.103333  21.196669  21.156668  24.093332\n",
      " 24.443335  25.789999  18.783335  30.31      26.42      28.653334\n",
      " 24.13      24.776667  24.946669  27.62      27.473333  24.936666\n",
      " 27.613333  30.83      29.343332  28.029999  28.236666  29.393333\n",
      " 29.050001  26.476667  30.673332  23.669998  25.166666  24.333334\n",
      " 29.196669  27.963333  22.596666  29.053335  28.773333  27.593332\n",
      " 24.76      22.086668  27.376665  26.123335  22.279999  30.450003\n",
      " 30.966667  28.58      30.4       34.239998  33.63      32.353336\n",
      " 29.973333  32.176666  29.203333  33.649998  30.096666  33.973335\n",
      " 29.296667  32.34667   32.11333   33.036667  33.556667  32.73\n",
      " 27.716667  31.303331  29.216667  32.843334  32.02667   31.62\n",
      " 33.213333  35.186665  36.166668  30.79      33.366665  37.396667\n",
      " 29.573334  29.746666  36.553333  35.783333  32.11      36.42\n",
      " 34.39      37.283337  35.100002  34.63      35.396664  37.513332\n",
      " 33.15333   37.903336  32.100002  37.393337  34.256668  41.803333\n",
      " 35.36333   37.28333   37.296665  36.796665  36.743332  32.476665\n",
      " 34.793335  32.213333  36.57      37.90333   37.923336  40.26\n",
      " 36.52333   33.186665  37.736668  35.73      30.456667  36.593334\n",
      " 36.210003  37.910004  38.566666  33.623333  40.710003  35.763336\n",
      " 42.52333   39.673332  36.266666  37.493336  35.156666  40.19\n",
      " 38.966667  36.393333  37.62      34.813335  34.393333  40.173336\n",
      " 38.873333  34.453335  31.946665  37.579998  36.37      40.75\n",
      " 36.903336  42.476665  33.00333   39.02      36.899998  41.583332\n",
      " 34.83      39.99      38.28      41.92      39.016666  37.59\n",
      " 39.716667  38.31667   41.15333   37.199997  39.61      43.036667\n",
      " 40.996662  41.916668  44.193333  36.28      39.        39.316666\n",
      " 34.546665  42.850002  41.53333   38.37      42.843334  44.349995\n",
      " 41.983334  36.72      38.296665  39.43333   42.006664  39.583332\n",
      " 37.423336  34.773335  38.        43.573334  39.329998  45.2\n",
      " 40.14333   42.296665  37.936665  38.36667   43.31667   40.813335\n",
      " 40.18      40.303333  40.733334  39.403336  42.606667  41.796665\n",
      " 41.40333   41.56      41.59      42.083332  40.039997  41.61\n",
      " 38.726665  42.45667   37.07      41.993332  36.11      40.546665\n",
      " 40.643333  45.410004  41.633335  42.086666  44.253338  39.113335\n",
      " 39.936665  40.46      37.656666  37.99      41.51      41.513332\n",
      " 43.116665  38.41      44.213333  41.88667   44.28333   38.31\n",
      " 37.686665  45.406666  40.97333   44.09      42.42667   43.936665\n",
      " 40.969997  38.123333  34.329998  40.763332  43.62      46.123333\n",
      " 48.23      43.583332  42.40333   46.343334  41.066666  43.49\n",
      " 42.73      41.77      47.083332  43.3       41.789997  40.71\n",
      " 44.2       38.54      42.063335  49.453335  40.463333  45.213333 ]\n",
      "3000 300 [10.       13.03     11.743333 13.273334 15.573334 15.206668 16.476667\n",
      " 17.673334 18.083334 19.25     19.94     21.050001 21.976667 22.840002\n",
      " 23.126665 24.446669 24.346666 23.160002 25.029999 26.413332 26.943335\n",
      " 26.696665 26.61     27.156668 26.643333 26.203333 27.69     29.626669\n",
      " 28.75     29.726664 29.593332 29.823334 29.12     30.086668 30.893333\n",
      " 30.733332 30.39     29.526667 30.833334 30.443335 31.313334 31.583334\n",
      " 31.966667 31.659998 31.766668 31.316668 31.623335 32.399998 31.813334\n",
      " 32.913334 33.100002 32.77667  34.083336 33.506668 33.843334 33.373333\n",
      " 34.023335 34.596664 33.53333  34.76     34.003334 35.47333  35.053333\n",
      " 34.61     35.026665 35.77667  37.183334 35.72333  36.239998 36.58\n",
      " 36.59667  36.11     37.11333  36.789997 37.313335 37.053333 38.243332\n",
      " 37.803333 36.986668 38.850002 38.98     37.030003 38.133335 39.24667\n",
      " 38.850002 39.116665 39.383335 40.196667 38.093334 40.58     40.326668\n",
      " 39.97333  39.293335 40.649998 40.34667  40.586666 40.48     41.573334\n",
      " 41.093334 40.993332 40.866665 41.646667 40.676666 40.683334 41.460003\n",
      " 42.420002 42.01     40.05     42.056667 42.08     43.35667  41.903336\n",
      " 42.976665 42.53     42.633335 43.74     43.079998 43.12     43.296665\n",
      " 42.103333 43.686665 42.670002 43.24667  43.36667  44.33     43.686665\n",
      " 44.28     43.34667  44.52     44.11     44.61     43.676666 44.566666\n",
      " 45.126667 44.5      44.099995 44.399998 44.213333 44.079998 45.87\n",
      " 44.293335 43.84     45.289997 45.606663 44.64     45.283337 44.843334\n",
      " 44.896667 44.63     45.233334 46.563335 43.600002 45.063335 45.316666\n",
      " 46.373337 44.876667 45.476665 46.783337 44.309998 45.190002 46.173336\n",
      " 45.13667  45.963337 46.063335 46.163334 46.676666 46.670002 46.\n",
      " 46.78     45.803333 45.303333 47.410004 45.440002 46.313335 47.703335\n",
      " 47.063335 46.460003 47.326664 47.22333  46.426666 47.333332 46.323334\n",
      " 47.40333  46.686665 46.783337 47.08     47.059998 46.21667  48.243336\n",
      " 46.34     47.22     47.033337 47.73     47.823334 47.243332 46.33\n",
      " 47.49     47.61333  47.559998 46.426666 47.243332 48.333332 47.84667\n",
      " 48.186665 47.783337 47.976665 48.59     47.673336 48.333332 48.186665\n",
      " 48.79333  46.97     48.483334 48.07667  48.726665 48.196667 47.706665\n",
      " 48.539997 48.95     48.546665 47.95     48.423336 48.696667 48.196667\n",
      " 48.156666 48.726665 48.666668 48.743332 48.2      48.83667  48.98\n",
      " 48.819996 48.066666 48.506668 48.413334 48.78     49.286667 49.423336\n",
      " 48.466663 49.606663 49.706665 49.293335 49.400005 50.149998 49.463333\n",
      " 48.186665 49.763332 49.543335 49.22     48.803333 49.333332 49.483334\n",
      " 50.036667 49.196667 49.430004 49.943333 49.583332 50.046665 49.666668\n",
      " 48.873333 49.680004 49.88666  49.673336 50.72     50.053333 50.42\n",
      " 49.73     50.796665 50.546665 50.706665 49.843334 50.68333  49.916668\n",
      " 50.306667 50.146667 50.77     50.14333  50.86333  50.74     50.316666\n",
      " 50.11     49.783337 50.28     50.559998 50.88     49.963333 50.880005\n",
      " 49.936665 51.710003 50.946667 51.47     50.61333  50.40333  51.350002\n",
      " 50.343334 50.87     50.48     51.210003 49.88     50.393337]\n",
      "3000 300 [ 9.996667 14.423332 16.813332 16.67     13.383334 19.283333 21.43\n",
      " 17.276667 15.860001 22.636667 18.793333 22.456667 19.11     24.846666\n",
      " 25.766668 25.26333  27.31     26.173334 27.276667 24.85     25.13\n",
      " 26.303335 27.236666 27.89     24.813334 28.356667 26.203333 27.51\n",
      " 29.12     30.68     28.606667 26.853333 30.713333 30.036667 28.616667\n",
      " 26.856667 29.686668 28.78     30.1      28.35     32.696667 30.856667\n",
      " 32.11333  31.513334 31.51     32.163334 31.503332 31.863333 31.263334\n",
      " 32.353336 32.85     33.333332 32.086666 33.77     32.816666 33.936665\n",
      " 32.423336 33.093334 30.476667 33.906666 34.046665 33.676666 33.993332\n",
      " 33.356667 32.626667 34.16333  32.803333 34.573334 32.986668 35.593334\n",
      " 34.123333 33.100002 36.36333  34.50333  33.316666 36.63667  34.86333\n",
      " 34.983334 35.093334 36.75     37.576668 37.006668 36.816666 36.863335\n",
      " 35.533333 36.316666 37.916668 37.02     37.92     38.82     38.023335\n",
      " 37.7      34.416668 37.086666 37.23     38.24667  38.103333 39.04\n",
      " 38.440002 39.85667  39.06     38.76     39.453335 38.713333 37.190002\n",
      " 39.82667  38.646664 40.34     40.71     39.86667  42.166668 39.02667\n",
      " 40.52333  40.196667 40.66     36.806667 40.24667  41.686665 41.476665\n",
      " 41.940002 43.430004 41.29333  40.77333  42.29     41.053333 43.32667\n",
      " 43.410004 42.513336 41.803333 40.913334 44.126667 41.45333  42.953335\n",
      " 43.59     42.416668 43.246662 44.680004 44.043335 44.173336 45.013336\n",
      " 41.680004 44.833332 43.02     45.19     44.413334 44.03     44.85667\n",
      " 45.176666 43.08     44.743336 43.75     45.156666 46.19     45.579998\n",
      " 45.11333  44.523335 45.626667 45.66     45.086666 45.993336 44.61\n",
      " 46.87     45.60667  45.003338 45.38333  43.13667  45.486664 45.006668\n",
      " 45.516666 45.466663 44.16     46.60667  45.84     46.539997 45.983334\n",
      " 46.323334 43.95     45.773335 46.67333  47.55     47.25     46.446667\n",
      " 45.906666 47.62     45.956665 46.11     47.103333 45.666668 46.170002\n",
      " 47.920002 46.10667  47.02666  48.27667  47.736664 45.623333 46.97333\n",
      " 47.83667  47.486664 46.123333 48.649998 48.266666 48.343334 46.466663\n",
      " 47.600002 47.306667 47.766666 48.02     47.726665 47.746662 48.506668\n",
      " 47.86333  48.76     48.149998 47.72     49.196667 47.876667 47.993332\n",
      " 47.763332 48.24667  48.303333 47.996662 48.953335 48.956665 47.853333\n",
      " 49.55     49.756668 49.563335 48.013336 46.68333  48.373333 47.226665\n",
      " 49.63333  46.576664 49.89333  49.28333  47.680004 49.583332 50.833332\n",
      " 50.11333  50.763336 49.313335 49.786667 47.47667  50.410004 49.576664\n",
      " 49.063335 49.03333  50.51     49.579998 50.190002 49.216663 46.056667\n",
      " 49.97     50.86     50.156666 49.940002 50.266666 50.03     49.63667\n",
      " 49.876663 50.61     49.97     50.716663 49.459995 50.05667  49.27667\n",
      " 49.83     50.463333 47.839996 48.016666 51.683334 50.10667  51.246662\n",
      " 51.28     50.866665 51.420002 48.8      51.60667  50.60667  51.67\n",
      " 50.25     49.923336 50.876667 50.716663 51.190002 50.703335 51.38\n",
      " 50.350002 50.87     49.61667  49.97333  51.376663 50.333332 51.33667\n",
      " 52.420002 50.823334 51.463333 50.786667 51.683334 51.496662]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/anaconda3/envs/fl/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dict_std_acc_test = {}\n",
    "plot(dict_acc_test1, dict_std_acc_test, 'test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
