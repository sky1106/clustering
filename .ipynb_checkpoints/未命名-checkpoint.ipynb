{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import color, transform   # lib: scikit-image\n",
    "import keras\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, cifar_noniid, fashion_mnist_iid, fashion_mnist_noniid\n",
    "from utils.lsh import LSHAlgo\n",
    "from models.Fed import FedAvg\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "user_feats = []\n",
    "print('Loading dataset...')\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "dict_users = mnist_noniid(dataset_train, 100, case=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(800, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_fed(dataset_train, dataset_test, dict_users, type_exp = 'base'):\n",
    "    img_size = dataset_train[0][0].shape\n",
    "\n",
    "    dict_clusters = {}\n",
    "    if type_exp == 'cluster' or type_exp == 'lsh-cluster':\n",
    "        # feature map\n",
    "        print('Featuring...')\n",
    "        input_shape = (max(img_size[1], 32), max(img_size[2], 32), max(img_size[0], 3))\n",
    "        model1 = keras.applications.resnet.ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "\n",
    "        if len(user_feats):\n",
    "            pass\n",
    "        else:\n",
    "            for idx_user in dict_users:\n",
    "                print('User', idx_user, 'featuring...')\n",
    "                user_images = []\n",
    "                for idx in dict_users[idx_user]:\n",
    "                    image = dataset_train[idx][0].numpy()\n",
    "                    image = color.gray2rgb(image)[0]\n",
    "                    image = transform.resize(image, (32, 32))\n",
    "                    user_images.append(image)\n",
    "\n",
    "                pred = model1.predict([user_images])\n",
    "                feats = np.mean([data[0][0] for data in pred], axis=0)\n",
    "                user_feats.append(feats)\n",
    "\n",
    "        if type_exp == 'lsh-cluster':\n",
    "            # 局部敏感哈希\n",
    "            print('LSH...')\n",
    "            lsh = LSHAlgo(feat_dim=len(user_feats[0]), code_dim=512) # code_dim: 输出维度\n",
    "            user_feats1 = lsh.run(user_feats)\n",
    "        else:\n",
    "            # 普通降维\n",
    "            print('PCA...')\n",
    "            pca = PCA(n_components=50, random_state=728)\n",
    "            user_feats1 = pca.fit_transform(user_feats)\n",
    "\n",
    "        # 聚类 users\n",
    "        print('Clustering...')\n",
    "        kmeans = KMeans(n_clusters=10, random_state=728)\n",
    "        kmeans.fit(user_feats1)\n",
    "\n",
    "        for idx_user, label in enumerate(kmeans.labels_):\n",
    "            if label in dict_clusters:\n",
    "                dict_clusters[label].append(idx_user)\n",
    "            else:\n",
    "                dict_clusters[label] = [idx_user]\n",
    "        print('Clustering finished.')\n",
    "        print('Dict of cluster - users: ', dict_clusters)\n",
    "\n",
    "\n",
    "    # build model\n",
    "    net_glob = CNNMnist().to('cuda:0')\n",
    "    print(net_glob)\n",
    "    net_glob.train()\n",
    "\n",
    "\n",
    "    # batch training\n",
    "\n",
    "    return batch_train(type_exp, net_glob, dataset_train, dataset_test, dict_users, dict_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(type_exp, net_glob, dataset_train, dataset_test, dict_users, dict_clusters):\n",
    "    loss_train_batch = []\n",
    "    acc_test_batch = []\n",
    "\n",
    "    for big_iter in range(5):\n",
    "        print('Iteration ', big_iter)\n",
    "\n",
    "        # copy weights\n",
    "        net_glob_copy = copy.deepcopy(net_glob)\n",
    "\n",
    "        # training\n",
    "        loss_train = []\n",
    "        acc_test = []\n",
    "\n",
    "        for iter in range(5):\n",
    "            one_loss_train, one_acc_test = train_one_round(iter, type_exp, net_glob_copy, dataset_train, dataset_test, dict_users, dict_clusters)\n",
    "            loss_train.append(one_loss_train)\n",
    "            acc_test.append(one_acc_test)\n",
    "\n",
    "        loss_train_batch.append(loss_train)\n",
    "        acc_test_batch.append(acc_test)\n",
    "\n",
    "    loss_train_avg = np.mean(loss_train_batch, axis=0)\n",
    "    acc_test_avg = np.mean(acc_test_batch, axis=0)\n",
    "\n",
    "    loss_train_std = np.std(loss_train_batch, axis=0)\n",
    "    acc_test_std = np.std(acc_test_batch, axis=0)\n",
    "\n",
    "    return loss_train_avg, acc_test_avg, loss_train_std, acc_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=128)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if False:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_round(iter, type_exp, net_glob, dataset_train, dataset_test, dict_users, dict_clusters):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    if type_exp == 'cluster' or type_exp == 'lsh-cluster':\n",
    "        # 预先聚类的情况\n",
    "        idxs_users = []\n",
    "        for idx_cluster in dict_clusters:\n",
    "            idxs_users += list(np.random.choice(list(dict_clusters[idx_cluster]), 1, replace=False))\n",
    "    else:\n",
    "        m = max(int(0.1 * 100), 1)\n",
    "        idxs_users = np.random.choice(range(100), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob_copy\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss & acc\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    one_acc_test, one_loss_test = test_img(net_glob, dataset_test)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Test accuracy {:.3f}'.format(iter, loss_avg, one_acc_test))\n",
    "\n",
    "    return loss_avg, one_acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalUpdate(object):\n",
    "    def __init__(self, dataset=None, idxs=None):\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=50, shuffle=True)\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        # train and update\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "        epoch_loss = []\n",
    "        for iter in range(5):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to('cuda:0'), labels.to('cuda:0')\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images)\n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if False and batch_idx % 10 == 0:\n",
    "                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n",
    "                               100. * batch_idx / len(self.ldr_train), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, data_std, ylabel):\n",
    "    plt.figure()    \n",
    "    # colour = ['darkblue','darkred','darkgreen','black','darkmagenta','darkorange','darkcyan']\n",
    "    # ecolour = ['cornflowerblue','lightcoral','lightgreen','gray','magenta','bisque','cyan']\n",
    "    # i = 0\n",
    "    for label in data:\n",
    "        plt.plot(range(len(data[label])), data[label], label=label, linestyle=':')\n",
    "        # i = i + 1\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig('./test/fed_{}_{}_{}_{}_{}_{}_{}_iid{}_{}.pdf'.format('all', ylabel, 'mnist', 'cnn', '1', '5', '5', 'noniid', datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin time:  2020-06-28-14-51-29\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Iteration  0\n",
      "Round   0, Average loss 0.210, Test accuracy 10.320\n",
      "Round   1, Average loss 0.095, Test accuracy 16.230\n",
      "Round   2, Average loss 0.101, Test accuracy 9.800\n",
      "Round   3, Average loss 0.170, Test accuracy 16.470\n",
      "Round   4, Average loss 0.147, Test accuracy 17.200\n",
      "Iteration  1\n",
      "Round   0, Average loss 0.211, Test accuracy 12.040\n",
      "Round   1, Average loss 0.096, Test accuracy 10.100\n",
      "Round   2, Average loss 0.084, Test accuracy 10.910\n",
      "Round   3, Average loss 0.084, Test accuracy 17.920\n",
      "Round   4, Average loss 0.090, Test accuracy 10.320\n",
      "Iteration  2\n",
      "Round   0, Average loss 0.222, Test accuracy 17.230\n",
      "Round   1, Average loss 0.115, Test accuracy 9.580\n",
      "Round   2, Average loss 0.125, Test accuracy 10.320\n",
      "Round   3, Average loss 0.090, Test accuracy 12.160\n",
      "Round   4, Average loss 0.120, Test accuracy 13.110\n",
      "Iteration  3\n",
      "Round   0, Average loss 0.281, Test accuracy 10.320\n",
      "Round   1, Average loss 0.120, Test accuracy 17.010\n",
      "Round   2, Average loss 0.085, Test accuracy 9.800\n",
      "Round   3, Average loss 0.163, Test accuracy 18.860\n",
      "Round   4, Average loss 0.119, Test accuracy 11.350\n",
      "Iteration  4\n",
      "Round   0, Average loss 0.278, Test accuracy 13.930\n",
      "Round   1, Average loss 0.100, Test accuracy 20.670\n",
      "Round   2, Average loss 0.152, Test accuracy 17.510\n",
      "Round   3, Average loss 0.084, Test accuracy 18.850\n",
      "Round   4, Average loss 0.050, Test accuracy 16.360\n",
      "Featuring...\n",
      "User 0 featuring...\n",
      "User 1 featuring...\n",
      "User 2 featuring...\n",
      "User 3 featuring...\n",
      "User 4 featuring...\n",
      "User 5 featuring...\n",
      "User 6 featuring...\n",
      "User 7 featuring...\n",
      "User 8 featuring...\n",
      "User 9 featuring...\n",
      "User 10 featuring...\n",
      "User 11 featuring...\n",
      "User 12 featuring...\n",
      "User 13 featuring...\n",
      "User 14 featuring...\n",
      "User 15 featuring...\n",
      "User 16 featuring...\n",
      "User 17 featuring...\n",
      "User 18 featuring...\n",
      "User 19 featuring...\n",
      "User 20 featuring...\n",
      "User 21 featuring...\n",
      "User 22 featuring...\n",
      "User 23 featuring...\n",
      "User 24 featuring...\n",
      "User 25 featuring...\n",
      "User 26 featuring...\n",
      "User 27 featuring...\n",
      "User 28 featuring...\n",
      "User 29 featuring...\n",
      "User 30 featuring...\n",
      "User 31 featuring...\n",
      "User 32 featuring...\n",
      "User 33 featuring...\n",
      "User 34 featuring...\n",
      "User 35 featuring...\n",
      "User 36 featuring...\n",
      "User 37 featuring...\n",
      "User 38 featuring...\n",
      "User 39 featuring...\n",
      "User 40 featuring...\n",
      "User 41 featuring...\n",
      "User 42 featuring...\n",
      "User 43 featuring...\n",
      "User 44 featuring...\n",
      "User 45 featuring...\n",
      "User 46 featuring...\n",
      "User 47 featuring...\n",
      "User 48 featuring...\n",
      "User 49 featuring...\n",
      "User 50 featuring...\n",
      "User 51 featuring...\n",
      "User 52 featuring...\n",
      "User 53 featuring...\n",
      "User 54 featuring...\n",
      "User 55 featuring...\n",
      "User 56 featuring...\n",
      "User 57 featuring...\n",
      "User 58 featuring...\n",
      "User 59 featuring...\n",
      "User 60 featuring...\n",
      "User 61 featuring...\n",
      "User 62 featuring...\n",
      "User 63 featuring...\n",
      "User 64 featuring...\n",
      "User 65 featuring...\n",
      "User 66 featuring...\n",
      "User 67 featuring...\n",
      "User 68 featuring...\n",
      "User 69 featuring...\n",
      "User 70 featuring...\n",
      "User 71 featuring...\n",
      "User 72 featuring...\n",
      "User 73 featuring...\n",
      "User 74 featuring...\n",
      "User 75 featuring...\n",
      "User 76 featuring...\n",
      "User 77 featuring...\n",
      "User 78 featuring...\n",
      "User 79 featuring...\n",
      "User 80 featuring...\n",
      "User 81 featuring...\n",
      "User 82 featuring...\n",
      "User 83 featuring...\n",
      "User 84 featuring...\n",
      "User 85 featuring...\n",
      "User 86 featuring...\n",
      "User 87 featuring...\n",
      "User 88 featuring...\n",
      "User 89 featuring...\n",
      "User 90 featuring...\n",
      "User 91 featuring...\n",
      "User 92 featuring...\n",
      "User 93 featuring...\n",
      "User 94 featuring...\n",
      "User 95 featuring...\n",
      "User 96 featuring...\n",
      "User 97 featuring...\n",
      "User 98 featuring...\n",
      "User 99 featuring...\n",
      "PCA...\n",
      "Clustering...\n",
      "Clustering finished.\n",
      "Dict of cluster - users:  {9: [0, 6, 13, 48, 50, 65, 66, 73, 79, 97], 1: [1, 12, 18, 25, 30, 31, 47, 53, 81, 87, 88], 8: [2, 16, 37, 45, 58, 68, 69, 72, 78, 96], 5: [3, 11, 22, 23, 39, 40, 61, 70, 82, 95], 0: [4, 9, 21, 32, 44, 63, 71, 80, 93, 94], 2: [5, 15, 19, 20, 27, 41, 57, 83, 86, 89], 7: [7, 8, 10, 24, 28, 52, 64, 67, 98, 99], 3: [14, 29, 33, 34, 46, 54, 56, 60, 84, 92], 6: [17, 38, 42, 49, 51, 55, 75, 76, 85, 90], 4: [26, 35, 36, 43, 59, 62, 74, 77, 91]}\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Iteration  0\n",
      "Round   0, Average loss 0.302, Test accuracy 10.320\n",
      "Round   1, Average loss 0.123, Test accuracy 10.390\n",
      "Round   2, Average loss 0.085, Test accuracy 16.160\n",
      "Round   3, Average loss 0.112, Test accuracy 23.940\n",
      "Round   4, Average loss 0.103, Test accuracy 28.810\n",
      "Iteration  1\n",
      "Round   0, Average loss 0.282, Test accuracy 10.160\n",
      "Round   1, Average loss 0.122, Test accuracy 14.700\n",
      "Round   2, Average loss 0.090, Test accuracy 14.050\n",
      "Round   3, Average loss 0.090, Test accuracy 27.290\n",
      "Round   4, Average loss 0.059, Test accuracy 30.590\n",
      "Iteration  2\n",
      "Round   0, Average loss 0.258, Test accuracy 10.100\n",
      "Round   1, Average loss 0.111, Test accuracy 10.910\n",
      "Round   2, Average loss 0.067, Test accuracy 23.870\n",
      "Round   3, Average loss 0.062, Test accuracy 16.290\n",
      "Round   4, Average loss 0.058, Test accuracy 42.330\n",
      "Iteration  3\n",
      "Round   0, Average loss 0.278, Test accuracy 10.130\n",
      "Round   1, Average loss 0.104, Test accuracy 9.750\n",
      "Round   2, Average loss 0.068, Test accuracy 17.750\n",
      "Round   3, Average loss 0.113, Test accuracy 17.370\n",
      "Round   4, Average loss 0.058, Test accuracy 22.680\n",
      "Iteration  4\n",
      "Round   0, Average loss 0.255, Test accuracy 10.100\n",
      "Round   1, Average loss 0.123, Test accuracy 10.160\n",
      "Round   2, Average loss 0.087, Test accuracy 23.880\n",
      "Round   3, Average loss 0.078, Test accuracy 18.630\n",
      "Round   4, Average loss 0.058, Test accuracy 37.870\n",
      "Featuring...\n",
      "LSH...\n",
      "Clustering...\n",
      "Clustering finished.\n",
      "Dict of cluster - users:  {8: [0, 6, 13, 48, 50, 65, 66, 73, 79, 97], 1: [1, 12, 18, 25, 30, 31, 47, 53, 81, 87, 88], 2: [2, 16, 37, 45, 58, 68, 69, 72, 78, 96], 5: [3, 11, 22, 23, 39, 40, 61, 70, 82, 95], 0: [4, 9, 21, 32, 44, 63, 71, 80, 93, 94], 3: [5, 15, 19, 20, 27, 41, 57, 83, 86, 89], 4: [7, 8, 10, 24, 28, 52, 64, 67, 98, 99], 6: [14, 29, 33, 34, 46, 54, 56, 60, 84, 92], 9: [17, 38, 42, 49, 51, 55, 75, 76, 85, 90], 7: [26, 35, 36, 43, 59, 62, 74, 77, 91]}\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Iteration  0\n",
      "Round   0, Average loss 0.208, Test accuracy 21.210\n",
      "Round   1, Average loss 0.097, Test accuracy 11.620\n",
      "Round   2, Average loss 0.067, Test accuracy 30.920\n",
      "Round   3, Average loss 0.085, Test accuracy 29.790\n",
      "Round   4, Average loss 0.078, Test accuracy 16.330\n",
      "Iteration  1\n",
      "Round   0, Average loss 0.231, Test accuracy 11.400\n",
      "Round   1, Average loss 0.069, Test accuracy 24.890\n",
      "Round   2, Average loss 0.060, Test accuracy 28.670\n",
      "Round   3, Average loss 0.088, Test accuracy 26.710\n",
      "Round   4, Average loss 0.084, Test accuracy 40.510\n",
      "Iteration  2\n",
      "Round   0, Average loss 0.204, Test accuracy 16.040\n",
      "Round   1, Average loss 0.068, Test accuracy 20.740\n",
      "Round   2, Average loss 0.060, Test accuracy 38.390\n",
      "Round   3, Average loss 0.059, Test accuracy 20.480\n",
      "Round   4, Average loss 0.092, Test accuracy 10.010\n",
      "Iteration  3\n",
      "Round   0, Average loss 0.202, Test accuracy 17.110\n",
      "Round   1, Average loss 0.069, Test accuracy 20.720\n",
      "Round   2, Average loss 0.090, Test accuracy 16.450\n",
      "Round   3, Average loss 0.086, Test accuracy 23.230\n",
      "Round   4, Average loss 0.080, Test accuracy 25.940\n",
      "Iteration  4\n",
      "Round   0, Average loss 0.202, Test accuracy 16.910\n",
      "Round   1, Average loss 0.121, Test accuracy 12.250\n",
      "Round   2, Average loss 0.082, Test accuracy 17.860\n",
      "Round   3, Average loss 0.086, Test accuracy 28.730\n",
      "Round   4, Average loss 0.087, Test accuracy 24.400\n",
      "{'base': array([0.24010662, 0.10528759, 0.10958549, 0.11822743, 0.10517779]), 'cluster': array([0.27510488, 0.11653744, 0.07944274, 0.09097647, 0.06733475]), 'lsh-cluster': array([0.20930415, 0.08474278, 0.07181644, 0.08091459, 0.08417415])} {'base': array([12.768   , 14.717999, 11.667999, 16.852001, 13.668001],\n",
      "      dtype=float32), 'cluster': array([10.162, 11.182, 19.142, 20.704, 32.456], dtype=float32), 'lsh-cluster': array([16.534   , 18.044   , 26.457998, 25.787998, 23.438   ],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print('begin time: ', datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "labels = ['base', 'cluster', 'lsh-cluster']\n",
    "dict_train_loss = {}\n",
    "dict_acc_test = {}\n",
    "dict_std_train_loss = {}\n",
    "dict_std_acc_test = {}\n",
    "for label in labels:\n",
    "    dict_train_loss[label], dict_acc_test[label], dict_std_train_loss[label], dict_std_acc_test[label] = run_fed(dataset_train, dataset_test, dict_users, type_exp = label)\n",
    "print(dict_train_loss, dict_acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(dict_train_loss, dict_std_train_loss, 'train_loss')\n",
    "plot(dict_acc_test, dict_std_acc_test, 'test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save finished\n"
     ]
    }
   ],
   "source": [
    "with open(r'./test/test.txt', 'a') as f:\n",
    "    for label in dict_acc_test:\n",
    "        f.write(label)\n",
    "        f.write(' ')\n",
    "        for item in dict_acc_test[label]:\n",
    "            item1 = str(item)\n",
    "            f.write(item1)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "print('save finished')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# for label in dict_acc_test:\n",
    "#     print(label)\n",
    "#     for item in dict_acc_test[label]:\n",
    "#         print(item)\n",
    "\n",
    "\n",
    "# for item in dict_acc_test.items():\n",
    "#     for i in range(len(item)):\n",
    "#         str1 = item[i]\n",
    "#         print(str1,end=' ')\n",
    "#         with open(r'./test/test.txt', 'a') as f:\n",
    "#             f.write(str1)\n",
    "#             f.write('\\r\\t')\n",
    "#         print('finish saving')    \n",
    "#         f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': array([12.768   , 14.717999, 11.667999, 16.852001, 13.668001],\n",
       "       dtype=float32),\n",
       " 'cluster': array([10.162, 11.182, 19.142, 20.704, 32.456], dtype=float32),\n",
       " 'lsh-cluster': array([16.534   , 18.044   , 26.457998, 25.787998, 23.438   ],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_acc_test1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4691c1e2c20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'./test/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_acc_test1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_acc_test1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_acc_test1' is not defined"
     ]
    }
   ],
   "source": [
    "with open(r'./test/test.txt', 'a') as f:\n",
    "    for label in dict_acc_test1:\n",
    "        f.write(label)\n",
    "        f.write(' ')\n",
    "        for item in dict_acc_test1[label]:\n",
    "            item1 = str(item)\n",
    "            f.write(item1)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "print('save finished')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = []\n",
    "items2 = []\n",
    "with open(r'./test/all_test_acc_cifar_cnn_1_3_3000_iidFalse_2020-06-27-09-40-48.txt', 'r') as f:\n",
    "    for item in f.readlines():\n",
    "        item1 = np.array(item.split())\n",
    "        label1 = item1[0]\n",
    "        labels1.append(label1)\n",
    "        item2 = np.delete(item1, 0).astype('float32')\n",
    "        items2.append(item2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': array([ 9.776667, 10.66    , 10.      , ..., 44.98    , 47.52    ,\n",
      "       41.88667 ], dtype=float32), 'cluster': array([10.      , 10.      , 11.276668, ..., 50.556667, 51.15333 ,\n",
      "       51.55    ], dtype=float32), 'lsh-cluster': array([ 9.996667, 10.166667, 10.496667, ..., 51.186665, 50.87    ,\n",
      "       52.18333 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "dict_acc_test1 = {}\n",
    "i=0\n",
    "for label in labels1:\n",
    "#     print(label)\n",
    "#     print(i)\n",
    "#     print(items2[i])\n",
    "    dict_acc_test1[label] = items2[i]\n",
    "    i = i+1\n",
    "print(dict_acc_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base', 'cluster', 'lsh-cluster']\n"
     ]
    }
   ],
   "source": [
    "print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_std_acc_test = {}\n",
    "plot(dict_acc_test1, dict_std_acc_test, 'test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
